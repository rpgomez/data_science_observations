{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "In this notebook I want to demonstrate how to compute the probability of an observed multinomial count $(C_1,C_2,\\ldots, C_n)$ for a categorical random variable $X$ whose probability distribution is modeled by a [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior) $Dir((\\alpha_1,\\ldots,\\alpha_n))$. The computation is relatively straightforward from the definition of the probability distribution for a Dirichlet distribution, but it's a good exercise to perform the computation. \n",
    "\n",
    "A second objective is to demonstrate that unlike the case of the space of probability distributions on a categorical variable $X$, there is **not** an equivalence of a [Maximum Likelihood Estimator (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood) over the space of possible Dirichlet Conjugate priors given observed multinomial count data. The reason this is of interest to me is that I would like to be able use [Wilks's theorem](https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.27s_theorem)  on spaces of Dirichlet Conjugate priors to determine from observed data whether or not a categorical variable's probability distribution is a cross product of 2 or more probability distributions. However, this is not possible as I will demonstrate below.\n",
    "\n",
    "### Dirichlet Distribution\n",
    "The Dirichlet distribution is a probability distribution *over* the space of probability distributions for a categorical variable $X$ which has $N$ distinct possible values. The Dirichlet distribution is useful for [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) in which we don't know the exact distribution of a categorical variable $X$, but for which we can estimate probabilities *on* the possible probabilities distributions on $X$ given observed data.\n",
    "\n",
    "So here's the probability distribution for the Dirichlet Distribution with parameters $(\\alpha_i)$:\n",
    "\n",
    "$$ Pr(p_1,p_2,\\ldots,p_N|Dir((\\alpha_1,\\ldots,\\alpha_N)) = \\frac1{B(\\alpha)}p_1^{\\alpha_1} p_2^{\\alpha_2}\\cdots p_N^{\\alpha_N}, \\qquad B(\\alpha) := \\frac{\\prod_{i=1}^N\\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^N \\alpha_i)} $$\n",
    "\n",
    "where $\\Gamma(x)$ is the [gamma function](https://en.wikipedia.org/wiki/Gamma_function).\n",
    "\n",
    "And because  $Dir((\\alpha_1,\\ldots,\\alpha_N))$ *is* a probability distribution we necessarily *must* have:\n",
    "\n",
    "\n",
    "$$ \\frac1{B(\\alpha)}\\int_{(p_1,\\ldots,p_N)} p_1^{\\alpha_1} p_2^{\\alpha_2}\\cdots p_N^{\\alpha_N} dp_1dp_2\\cdots dp_N = 1, \\qquad  B(\\alpha) = \\int_{(p_1,\\ldots,p_N)} p_1^{\\alpha_1} p_2^{\\alpha_2}\\cdots p_N^{\\alpha_N} dp_1dp_2\\cdots dp_N $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute a probability on observed data.\n",
    "So given that we have a particular conjugate prior $Dir((\\alpha_i))$ for a categorical variable $X$, how do we compute the probability that we observe a multinomial count $(C_1,\\ldots,C_n)$? The idea is relatively simple. Start with an arbitrary probability distribution $(p_1,\\ldots,p_N)$ on $X$. Assume it's the true distribution on $X$. What is the probability of observing $(C_1,\\ldots,C_N)$? \n",
    "\n",
    "Well it's just a multinomial distribution problem, and the probability is given by:\n",
    "\n",
    "$$Pr(C_1,\\ldots,C_N | p_1,\\ldots,p_N) = \\prod_{i=1}^N p_i^{C_i} $$\n",
    "\n",
    "*Now* we consider what is the probability that the arbitrary probability distribution $(p_1,\\ldots,p_N)$ was in fact correct? Without prior information on the true probability distribution, the best that we can do is assign it the weight given by the Dirichlet conjugate prior: \n",
    "\n",
    "$$ Pr(p_1,\\ldots,p_N|Dir((\\alpha_1,\\ldots,\\alpha_N)) = \\frac1{B(\\alpha)}p_1^{\\alpha_1} p_2^{\\alpha_2}\\cdots p_N^{\\alpha_N} $$\n",
    "\n",
    "Now we have a means of computing the observed multinomial count:\n",
    "\n",
    "\\begin{align}\n",
    "Pr\\bigl((C_1,\\ldots,C_N)| Dir(\\alpha_1,\\ldots, \\alpha_N)\\bigr) &= \\int_{(p_1,\\ldots,p_N)} Pr\\bigl((C_1,\\ldots,C_N), (p_1,\\ldots,p_N) | Dir(\\alpha_1,\\ldots, \\alpha_N)\\bigr) dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\int_{(p_1,\\ldots,p_N)} Pr\\bigl((C_1,\\ldots,C_N)| (p_1,\\ldots,p_N), Dir(\\alpha_1,\\ldots, \\alpha_N)\\bigr) Pr\\bigl((p_1,\\ldots,p_N)|Dir(\\alpha_1,\\ldots,\\alpha_N)\\bigr)dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\int_{(p_1,\\ldots,p_N)} Pr\\bigl((C_1,\\ldots,C_N)| (p_1,\\ldots,p_N)\\bigr) Pr\\bigl((p_1,\\ldots,p_N)|Dir(\\alpha_1,\\ldots,\\alpha_N)\\bigr)dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\frac1{B(\\alpha)}\\int_{(p_1,\\ldots,p_N)}  \\prod_{i=1}^N p_i^{C_i} \\prod_{i=1}^N p_i^{\\alpha_i} dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\frac1{B(\\alpha)}\\int_{(p_1,\\ldots,p_N)}  \\prod_{i=1}^N p_i^{C_i+\\alpha_i} dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\frac1{B(\\alpha)}B(\\alpha + C) \\\\\n",
    "&= \\frac{B(\\alpha + C)}{B(\\alpha)} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonexistence of Maximum Likelihood Estimator\n",
    "Now  given observed multinomial counts $(C_1,\\ldots,C_N)$ I want to know what are the parameters $(\\alpha_1,\\ldots,\\alpha_N)$ that maximize the observed data. Are there values for $\\alpha$ which realize the maximal value? I know that for a scalar $\\lambda \\gg 1$ that parameters $\\lambda \\alpha$ and $\\alpha$ have the same mean, but the variance for the former is much smaller, most of the weight is given to a small neighborhood of the sample mean. \n",
    "\n",
    "*If* $B(\\alpha + C)/B(\\alpha)$ is always less than the probability of the sample probability model then there is no optimal value of $\\alpha$. That would imply I can't apply Wilks's theorem on Dirichlet Conjugate prior distributions, because I can't find optimal parameters for the null or alternative hypotheses. \n",
    "\n",
    "Here's a demonstration of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a well known result that given multinomial counts data $(C_1,\\ldots,C_N)$ the probability distribution on $X$ that is the MLE is given by $p'(i) := C_i/\\sum_j C_j$, i.e., the sample probability distribution.\n",
    "\n",
    "In other words $Pr(data|p') \\ge Pr(data|p) \\quad \\forall p$ possible distributions on $X$. This implies\n",
    "\n",
    "\\begin{align}\n",
    "Pr\\bigl((C_1,\\ldots,C_N)| Dir(\\alpha_1,\\ldots, \\alpha_N)\\bigr) &= \\int_{(p_1,\\ldots,p_N)} Pr\\bigl((C_1,\\ldots,C_N), (p_1,\\ldots,p_N) | Dir(\\alpha_1,\\ldots, \\alpha_N)\\bigr) dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\int_{(p_1,\\ldots,p_N)} Pr\\bigl((C_1,\\ldots,C_N)| (p_1,\\ldots,p_N), Dir(\\alpha_1,\\ldots, \\alpha_N)\\bigr) Pr\\bigl((p_1,\\ldots,p_N)|Dir(\\alpha_1,\\ldots,\\alpha_N)\\bigr)dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\int_{(p_1,\\ldots,p_N)} Pr\\bigl((C_1,\\ldots,C_N)| (p_1,\\ldots,p_N)\\bigr) Pr\\bigl((p_1,\\ldots,p_N)|Dir(\\alpha_1,\\ldots,\\alpha_N)\\bigr)dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\frac1{B(\\alpha)}\\int_{(p_1,\\ldots,p_N)}  \\prod_{i=1}^N p_i^{C_i} \\prod_{i=1}^N p_i^{\\alpha_i} dp_1dp_2\\cdots dp_N \\\\\n",
    "&\\le \\frac1{B(\\alpha)}\\int_{(p_1,\\ldots,p_N)}  \\prod_{i=1}^N {p'}_i^{C_i}\\prod_{i=1}^N p^{\\alpha_i} dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\frac1{B(\\alpha)}\\prod_{i=1}^N {p'}_i^{C_i}\\int_{(p_1,\\ldots,p_N)}  \\prod_{i=1}^Np^{\\alpha_i} dp_1dp_2\\cdots dp_N \\\\\n",
    "&= \\frac{\\bigl(\\prod_{i=1}^N {p'}_i^{C_i}\\bigr)B(\\alpha)}{B(\\alpha)}\\\\\n",
    "&= \\prod_{i=1}^N {p'}_i^{C_i} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have  an upperbound on the possible maximal probability of the observed data for any Dirichlet Conjugate prior distribution. Because $\\alpha$ consists of finite values, the inequality is a strict inequality, (choose $p \\ne p'$ and a ball of radius $\\epsilon$ around $p$ that does not include $p'$ then the inequality is strict in that ball and so the integral sum contribution from that ball will necessarily be less than the supremum).  This is why you can't use Wilks's theorem on Dirichlet conjugate prior distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
