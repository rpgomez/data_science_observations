{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "=======\n",
    "\n",
    "In this notebook I want to derive an algorithm for detecting [stop words](https://en.wikipedia.org/wiki/Stop_words). My previous algorithm was to perform a simple frequency count of words in a corpus of documents and find-the-elbow in the plot of sorted counts to detect the stops words. This was effective in detecting a majority (~80%) of words that are considered to be stop words. However,\n",
    "a second filter might be more effective and detecting stop words.\n",
    "\n",
    "The idea is this: stop words are *syntactic sugar*, that is, they are necessary for writing grammatically correct sentences, but in of themselves they don't convey that much information. An alternative filter is to consider the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the conditional probability $$Pr(\\text{next word }| \\text{stop word})$$ The assertion is that for stop words the entropy should be *high* while for words that are not stop words the entropy should be lower.\n",
    "\n",
    "**Results**: I compared stop words determined by entropy vs. frequency counts and find that frequency counts is a more accurate algorithm, while using entropy as a secondary filter can result in some false negatives for stop words, and lastly, using a normal model is more stable at finding the elbow in a curve than the geometrical method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the probability distribution for a categorical variable $X$ with sparse data.\n",
    "\n",
    "In this case the $X$ is the variable representing the word that follows a candidate stop word. Since the size of the vocabulary of words is much larger than the set of words that will follow a candidate stop word I will take advantage of my work in this [notebook](https://github.com/rpgomez/data_science_observations/blob/master/Variable%20Order%20Markov%20Model.ipynb) to estimate the conditional probability $$Pr(\\text{next word }| \\text{stop word})$$ as follows:\n",
    "\n",
    "For a given candidate stop word,\n",
    "\n",
    "* take the observed frequency counts of words that follow, $\\{C_{j}| j \\in \\text{vocabulary}\\}$\n",
    "* Compute the mixing weights, $\\omega, 1- \\omega$ for the model $X = \\begin{cases} Y & Pr(X=Y) := \\omega \\\\\n",
    "Z & Pr(X=Z) = 1 - \\omega\\end{cases}$ where $$\\omega = \\frac{1  + \\sum_j  C_j}{2 + \\sum_j C_j}$$\n",
    "* Compute the probability distribution for $Pr(Y=j)$:\n",
    "$$ Pr(Y=j) = \\frac{C_j}{\\sum_k C_k}$$\n",
    "\n",
    "Then compute the Shannon Entropy for $X$ as follows:\n",
    "\n",
    "\\begin{align}\n",
    "Entropy &= -\\left(\\sum_{X=Y=j} \\omega Pr(j)\\log(\\omega Pr(j))\\right) - (1-\\omega)\\log(1-\\omega) \\\\\n",
    "&= -\\omega\\sum_{X=Y=j} Pr(j)\\log(\\omega Pr(j)) - (1-\\omega)\\log(1-\\omega) \\\\\n",
    "&= -\\omega\\left(\\sum_{X=Y=j} Pr(j)\\log(\\omega) + \\log(Pr(j))\\right) - (1-\\omega)\\log(1-\\omega) \\\\\n",
    "&= -\\omega\\log(\\omega) -\\omega\\left(\\sum_{X=Y=j}Pr(j)\\log(Pr(j))\\right) - (1-\\omega)\\log(1-\\omega) \\\\\n",
    "&= -\\omega\\left(\\sum_{X=Y=j}Pr(j)\\log(Pr(j))\\right) - \\omega\\log(\\omega) -  (1-\\omega)\\log(1-\\omega) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Moby Dick\n",
    "I'm using the entire corpus of Moby Dick to see if I can detect the English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ModuleNotFoundError:\n",
    "    # We don't have BeautifulSoup installed. We also need the lxml module\n",
    "    !pip install bs4 lxml\n",
    "    from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code to compute conditional probabilities and log likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_counts_dictionary(observed_sequence,d=4,debug=False):\n",
    "    \"\"\"Constructs the dictionary of observed counts of words following \n",
    "    contexts at most d words long. \n",
    "    \n",
    "    observed_sequence should be a sequence of type integer of non-negative integers.\n",
    "    Each integer corresponds to some word in the vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not debug:\n",
    "        def use_me(x):\n",
    "            return x\n",
    "    else:\n",
    "        use_me = tqdm\n",
    "            \n",
    "    counts = defaultdict(Counter)\n",
    "    N = len(observed_sequence)\n",
    "    \n",
    "    for t in use_me(range(N)):\n",
    "        obs = observed_sequence[t]\n",
    "        for l in range(0,d+1):\n",
    "            if t - l < 0:\n",
    "                break\n",
    "            context = observed_sequence[t-l:t]\n",
    "            counts[context][obs] +=1\n",
    "    \n",
    "    return counts\n",
    "            \n",
    "def construct_prob_dictionary(counts_dictionary,vocabulary_size,debug=False):\n",
    "    \"\"\"Computes the memory efficient dictionary word| context probabilities\"\"\"\n",
    "\n",
    "    V = vocabulary_size\n",
    "    \n",
    "    if not debug:\n",
    "        def use_me(x):\n",
    "            return x\n",
    "    else:\n",
    "        use_me = tqdm\n",
    "            \n",
    "    probs_dictionary = dict()\n",
    "    for context in use_me(counts_dictionary):\n",
    "        local_list = counts_dictionary[context]\n",
    "        total_count = sum(list(local_list.values()))\n",
    "        \n",
    "        R = len(local_list)\n",
    "        if R == V:\n",
    "            omega = 1.0\n",
    "        else:\n",
    "            omega = (1+ total_count)/(2 + total_count)\n",
    "            \n",
    "        probs_dictionary[context] = defaultdict(float)\n",
    "        \n",
    "        local_probs = defaultdict(float)\n",
    "        for word in local_list:\n",
    "            c_i = local_list[word]\n",
    "            pr = c_i/total_count\n",
    "            local_probs[word] = pr\n",
    "        \n",
    "        if R < V:\n",
    "            local_probs[None] = (1.0 - omega)/(V-R)\n",
    "        else:\n",
    "            local_probs[None] = 0.0\n",
    "        \n",
    "        probs_dictionary[context] = local_probs\n",
    "    \n",
    "    return probs_dictionary\n",
    "\n",
    "def generate_log_pdf_dict(probs_dictionary,debug=False):\n",
    "    \"\"\"Computes the log likelihood dictionary\n",
    "    log (Pr(symbol|context)) from the probs_dictionary\"\"\"\n",
    "    \n",
    "    if not debug:\n",
    "        def use_me(x):\n",
    "            return x\n",
    "    else:\n",
    "        use_me = tqdm\n",
    "    \n",
    "    log_pdf_dict = {}\n",
    "    for context in use_me(probs_dictionary):\n",
    "        local_pdf_list = probs_dictionary[context]\n",
    "        local_log_pdf_list = {}\n",
    "        for asymbol in local_pdf_list:\n",
    "            local_log_pdf_list[asymbol] = np.log2(local_pdf_list[asymbol])\n",
    "            \n",
    "        log_pdf_dict[context] = local_log_pdf_list\n",
    "    \n",
    "    return log_pdf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding-the-elbow algorithms:\n",
    "\n",
    "Here is code for 2 different algorithms for finding the elbow in a concave up/down graph.\n",
    "\n",
    "1. The first method guesses the split point and assumes the left and right sides can be modeled as\n",
    "normally distributed populations with different means and variances. It then find the splits that maximizes\n",
    "the likelihood of the split. That's where the elbow is located.\n",
    "\n",
    "2. The second method is a geometrical techique: draw a line segment connecting the ends of the graph and find\n",
    "the point that is furthest in the Euclidean distance sense from the line. That's the elbow.\n",
    "\n",
    "***Weaknesses:*** the location of an elbow should be insensitive to the addition or removal of intervals at the end of the graph. Neither of the 2 techniques is completely insensitive to this, but they can be relatively insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elbow_norm(S_n, width=3000,debug=False):\n",
    "    \"\"\"Assumes 2 species of sequences, each fitting \n",
    "    a model y = ax + b + e_i, e_i ~ Norm(0,sigma_i)\n",
    "    where x is index n and y_n = log(S_{n})\n",
    "    \n",
    "    Then y_{n+1} - y_n = log(S_{n+1}/S_n) = a(n+1 - n) + b - b + e_{n+1} + e_{n}\n",
    "                                          = a + f_{n+1} ~ Norm(a,sqrt(2) sigma_i)\n",
    "\n",
    "    This function finds the split by finding the MLE split.\"\"\"\n",
    "    def loglikely(ys,mu,sigma):\n",
    "        \"\"\"Computes log(Pr(ys)|Norm(mu,sigma))\"\"\"\n",
    "        constant = (np.log(sigma) + 0.5*np.log(np.pi*2))*ys.shape[0]\n",
    "        logl = -0.5*((ys-mu)**2).sum()/sigma**2 - constant\n",
    "        return logl\n",
    "    \n",
    "    # Takes the last [width] entries of the sequence to find the split location.\n",
    "    S = S_n[-1:-width:-1]\n",
    "    logS = np.log(S)\n",
    "    logSS = logS[1:] - logS[:-1]\n",
    "    \n",
    "    N = logSS.shape[0]\n",
    "    scores = np.zeros(N-1)\n",
    "    scores[:2] = -np.inf\n",
    "    for guess in range(2,N-1):\n",
    "        popA = logSS[:guess]\n",
    "        popB = logSS[guess:]\n",
    "        popA_mu = popA.mean()\n",
    "        popA_std = popA.std()\n",
    "        popB_mu = popB.mean()\n",
    "        popB_std = popB.std()\n",
    "        \n",
    "        scores[guess] = loglikely(popA,popA_mu, popA_std) + loglikely(popB,popB_mu,popB_std)\n",
    "    \n",
    "    scores[np.isnan(scores)] = -np.inf\n",
    "    split = scores.argmax() + 1\n",
    "    if debug:\n",
    "        return scores\n",
    "    return -split\n",
    "        \n",
    "\n",
    "def find_elbow(S_n):\n",
    "    \"\"\"Finds index i where the elbow occurs using the algorithm\n",
    "    the elbow is the point of the graph furthest from the line\n",
    "    connecting the end points. \n",
    "    \n",
    "    This technique is inspired by Rolle's theorem and the mean \n",
    "    value theorem. We take the graph and rotate it until it's \n",
    "    starting and end point are connected by a horizontal line\n",
    "    segment, then we find the extremal location.\"\"\"\n",
    "    \n",
    "    S0 = S_n[0]\n",
    "    SN = S_n[-1]\n",
    "    N = len(S_n) - 1\n",
    "    m = (SN - S0)/N\n",
    "    x = np.arange(S_n.shape[0])\n",
    "    \n",
    "    y = np.abs(S_n  - S0 - m*x)\n",
    "    i = y.argmax()\n",
    "    return i\n",
    "\n",
    "def stability_counts(sorted_list,min_interval=1000,max_interval=3000):\n",
    "    \"\"\"For each tail end sequence  of length t= min_interval,...,max_interval\n",
    "    the code will recover the elbow and return the found elbows for each\n",
    "    possible interval to determine if the elbow is stable.\"\"\"\n",
    "    \n",
    "    elbows = np.array([find_elbow(sorted_list[:-t:-1]) for t in \\\n",
    "                         range(min_interval, max_interval)])\n",
    "    return elbows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and parsing the Moby Dick content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Mody Dick as html and then extract the content\n",
    "!wget -c \"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\"\n",
    "htmltxt= open('2701-h.htm','r').read()\n",
    "soup = BeautifulSoup(htmltxt,'lxml')\n",
    "content = soup.text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to remove punctuation and non-word content as I'm only interested in modeling word content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = string.ascii_letters\n",
    "def filter_nonletters(aletter):\n",
    " if aletter in filter:\n",
    "    return aletter\n",
    " else:\n",
    "    return \" \"\n",
    "new_content = \"\".join([filter_nonletters(x) for x in tqdm(content)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll extract the words from the filtered content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(new_content)\n",
    "print(\"Length of word sequence: \",len(words))\n",
    "\n",
    "vocabulary = set(words)\n",
    "print(\"Size of vocabulary: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to construct the conditional probabilities and log probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_children = construct_counts_dictionary(tuple(words),d=1,debug=False)\n",
    "\n",
    "probs_words = construct_prob_dictionary(words_children,len(vocabulary),debug=False)\n",
    "\n",
    "logprob_words = generate_log_pdf_dict(probs_words,debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to construct the entropies for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = {}\n",
    "for aword in probs_words:\n",
    "    w = sum(list(words_children[aword].values()))\n",
    "    w = (1+w)/(2+w)\n",
    "    constant_term = -(w *np.log2(w) + (1-w)*np.log2(1-w))\n",
    "    my_probs = probs_words[aword]\n",
    "    my_logprobs = logprob_words[aword]\n",
    "    my_entropy = -sum([my_probs[next_word]*my_logprobs[next_word] for next_word in my_logprobs \\\n",
    "                      if next_word != None])\n",
    "    entropies[aword] = w*my_entropy + constant_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_entropies = np.array(list(entropies.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Graphs\n",
    "Here we're going to plot the recovered entropies of the conditional probabilities $Pr(next|word)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_entropies = np.sort(flattened_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(16,6))\n",
    "subplot(1,2,1)\n",
    "title(\"Distribution of Word Entropies:\")\n",
    "xlabel(\"Shannon Bit Entropy\")\n",
    "hist(flattened_entropies,bins=100);\n",
    "\n",
    "subplot(1,2,2)\n",
    "title(\"Sorted Word Entropies\")\n",
    "ylabel(\"Shannon Bit Entropy\")\n",
    "plot(sorted_entropies);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That rising peak on the right side of the right graph is what I claim to be candidate stop words. We're going to take the last 1000 entropies and see how close the 2 find-the-elbow algorithms agree on the location of the elbow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_norm = find_elbow_norm(sorted_entropies,width=1000)\n",
    "elbow_euclid = - find_elbow(sorted_entropies[-1:-1000:-1])\n",
    "\n",
    "print(\"Location of the elbow according to normal distribution: \", elbow_norm)\n",
    "print(\"Location of the elbow according to geometrical method:  \", elbow_euclid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first method believes the elbow occurs 96 entries from the right, while the geometrical method says it's 157 entries from the right. \n",
    "\n",
    "Let's see how stable those values are. I'm going to vary the width that I'm willing to consider as an interval of the  last N entries where $N = 500,\\ldots, 2000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_norm = np.array([find_elbow_norm(sorted_entropies,width=N) for N in tqdm(range(500,2000))])\n",
    "indices_euclidean = -stability_counts(sorted_entropies,min_interval=500,max_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(16,5))\n",
    "subplot(1,2,1)\n",
    "title(\"Histogram of elbows according to the normal model:\")\n",
    "xlabel('elbow location')\n",
    "hist(indices_norm,bins=np.arange(indices_norm.min(),indices_norm.max()+1));\n",
    "subplot(1,2,2)\n",
    "title(\"Histogram of elbows according to the Euclidean method:\")\n",
    "xlabel('elbow location')\n",
    "hist(indices_euclidean,bins=np.arange(indices_euclidean.min(),indices_euclidean.max()+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the normal model is more stable, but it is not completely insensitive to the interval size. Let's see if the 2 techniques have any elbow locations in common or at least most in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "their_norm_locs= np.argwhere(bincount(-indices_norm)>0).flatten()\n",
    "their_norm_counts = bincount(-indices_norm)[their_norm_locs]\n",
    "\n",
    "for x,y in zip(their_norm_locs,their_norm_counts):\n",
    "    print(\"loc: {0:4d} count: {1:4d}\".format(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "their_euclidean_locs= np.argwhere(bincount(-indices_euclidean)>0).flatten()\n",
    "their_euclidean_counts = bincount(-indices_euclidean)[their_euclidean_locs]\n",
    "\n",
    "for x,y in zip(their_euclidean_locs,their_euclidean_counts):\n",
    "    print(\"loc: {0:4d} count: {1:4d}\".format(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the 2 methods seem to agree roughly around an elbow location of 95-96 from the end of the right side. However, they seem to agree more strongly around elbow location of 155-157 from the right side. Let's see what kind of vocabulary is labelled as being stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab96 = [aword[0] for aword in entropies if entropies[aword]>= sorted_entropies[-96] and aword != ()]\n",
    "vocab155 = [aword[0] for aword in entropies if entropies[aword]>= sorted_entropies[-155] and aword != ()]\n",
    "\n",
    "vocab96.sort()\n",
    "vocab155.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(vocab96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words such as *ahab, whale, whales, men, sea, ship* should probably not be considered stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(vocab155))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition words like Starbuck, dead, fish, flask, queequeg should probably not be considered stop words either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Count Graphs\n",
    "Now let's see what happens when we consider frequency counts of words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(words)\n",
    "\n",
    "their_counts = np.array([avalue for avalue in word_counts.values()])\n",
    "sorted_counts = their_counts.copy()\n",
    "sorted_counts.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(16,6))\n",
    "subplot(1,2,1)\n",
    "title(\"Distribution of Frequency Counts:\")\n",
    "xlabel(\"Frequency Counts\")\n",
    "hist(sorted_counts[-2000:],bins=100);\n",
    "\n",
    "subplot(1,2,2)\n",
    "title(\"Sorted Frequency Counts\")\n",
    "ylabel(\"Frequency Counts\")\n",
    "plot(sorted_counts[-2000:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_norm = find_elbow_norm(sorted_counts,width=1000)\n",
    "elbow_euclid = - find_elbow(sorted_counts[-1:-1000:-1])\n",
    "\n",
    "print(\"Location of the elbow according to normal distribution: \", elbow_norm)\n",
    "print(\"Location of the elbow according to geometrical method:  \", elbow_euclid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_norm = np.array([find_elbow_norm(sorted_counts,width=N) for N in tqdm(range(500,2000))])\n",
    "indices_euclidean = -stability_counts(sorted_counts,min_interval=500,max_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(16,5))\n",
    "subplot(1,2,1)\n",
    "title(\"Histogram of elbows according to the normal model:\")\n",
    "xlabel('elbow location')\n",
    "hist(indices_norm,bins=np.arange(indices_norm.min(),indices_norm.max()+1));\n",
    "subplot(1,2,2)\n",
    "title(\"Histogram of elbows according to the Euclidean method:\")\n",
    "xlabel('elbow location')\n",
    "hist(indices_euclidean,bins=np.arange(indices_euclidean.min(),indices_euclidean.max()+1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "their_norm_locs= np.argwhere(bincount(-indices_norm)>0).flatten()\n",
    "their_norm_counts = bincount(-indices_norm)[their_norm_locs]\n",
    "\n",
    "for x,y in zip(their_norm_locs,their_norm_counts):\n",
    "    print(\"loc: {0:4d} count: {1:4d}\".format(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "their_euclidean_locs= np.argwhere(bincount(-indices_euclidean)>0).flatten()\n",
    "their_euclidean_counts = bincount(-indices_euclidean)[their_euclidean_locs]\n",
    "\n",
    "for x,y in zip(their_euclidean_locs,their_euclidean_counts):\n",
    "    print(\"loc: {0:4d} count: {1:4d}\".format(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the norm model technique is much more insensitive to the width interval. Having said that, the Euclidean method seems to be producing more conservative estimates on the location of the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab40 = [aword for aword in word_counts if word_counts[aword]>= sorted_counts[-40] and aword != ()]\n",
    "vocab78 = [aword for aword in word_counts if word_counts[aword]>= sorted_counts[-78] and aword != ()]\n",
    "\n",
    "vocab40.sort()\n",
    "vocab78.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(vocab40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(vocab78))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersections\n",
    "Let's see what words are in common between entropy and frequency counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(list1,list2):\n",
    "    \"\"\"Takes the intersection of 2 list of words and returns the words in common.\"\"\"\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    common = list(set1.intersection(set2))\n",
    "    common.sort()\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entropy 96 and Counts 40:\\n\", \" \".join(intersect(vocab96,vocab40)))\n",
    "print()\n",
    "print(\"entropy 96 and Counts 78:\\n\", \" \".join(intersect(vocab96,vocab78)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entropy 155 and Counts 40:\\n\", \" \".join(intersect(vocab155,vocab40)))\n",
    "print()\n",
    "print(\"entropy 155 and Counts 78:\\n\", \" \".join(intersect(vocab155,vocab78)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity which words were excluded from the stop words generated by the norm model of the frequency counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(vocab78).difference(vocab155))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would consider those words stop words as well, but I suppose it's better to have false negatives than false positives for stop words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
